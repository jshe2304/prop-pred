{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "295bd7c6-7848-4d00-a58f-02cbb9b16b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87216317-adeb-43ad-9941-58a4d0753bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianKernel(nn.Module):\n",
    "    def __init__(self, K=128, num_pair=512, std_width=1.0, start=0.0, stop=9.0):\n",
    "        super().__init__()\n",
    "        self.K = K\n",
    "        std_width = std_width\n",
    "        start = start\n",
    "        stop = stop\n",
    "        mean = torch.linspace(start, stop, K)\n",
    "        self.std = (std_width * (mean[1] - mean[0]))\n",
    "        self.register_buffer(\"mean\", mean)\n",
    "        self.mul = Embedding(num_pair, 1, padding_idx=0)\n",
    "        self.bias = Embedding(num_pair, 1, padding_idx=0)\n",
    "        nn.init.constant_(self.bias.weight, 0)\n",
    "        nn.init.constant_(self.mul.weight, 1.0)\n",
    "\n",
    "    def gaussian(self, x):\n",
    "        return torch.exp(\n",
    "            -0.5 * (((x - self.mean) / self.std) ** 2)\n",
    "        ) / (self.std * (2 * torch.pi) ** 0.5)\n",
    "\n",
    "    def forward(self, x, atom_pair):\n",
    "        mul = self.mul(atom_pair).abs().sum(dim=-2)\n",
    "        bias = self.bias(atom_pair).sum(dim=-2)\n",
    "        x = mul * x.unsqueeze(-1) + bias\n",
    "        x = x.expand(-1, -1, -1, self.K)\n",
    "        mean = self.mean.float().view(-1)\n",
    "        return gaussian(x.float(), mean, self.std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9d8cef-c9b0-4ce3-be35-5d75f9b79651",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nn.Linear):\n",
    "    def __init__(self, d_in, d_out, bias, init='relu'):\n",
    "        super(Linear, self).__init__(d_in, d_out, bias=bias)\n",
    "\n",
    "        self.use_bias = bias\n",
    "\n",
    "        if self.use_bias: \n",
    "            with torch.no_grad():\n",
    "                self.bias.fill_(0)\n",
    "\n",
    "        assert init == 'relu'\n",
    "        self._trunc_normal_init(2.0)\n",
    "\n",
    "    def _trunc_normal_init(self, scale=1.0):\n",
    "        # Constant from scipy.stats.truncnorm.std(a=-2, b=2, loc=0., scale=1.)\n",
    "        TRUNCATED_NORMAL_STDDEV_FACTOR = 0.87962566103423978\n",
    "        _, fan_in = self.weight.shape\n",
    "        scale = scale / max(1, fan_in)\n",
    "        std = (scale**0.5) / TRUNCATED_NORMAL_STDDEV_FACTOR\n",
    "        nn.init.trunc_normal_(self.weight, mean=0.0, std=std)\n",
    "\n",
    "    def _glorot_uniform_init(self):\n",
    "        nn.init.xavier_uniform_(self.weight, gain=1)\n",
    "\n",
    "    def _zero_init(self, use_bias=True):\n",
    "        with torch.no_grad():\n",
    "            self.weight.fill_(0.0)\n",
    "            if use_bias:\n",
    "                with torch.no_grad():\n",
    "                    self.bias.fill_(1.0)\n",
    "\n",
    "    def _normal_init(self):\n",
    "        torch.nn.init.kaiming_normal_(self.weight, nonlinearity=\"linear\")\n",
    "\n",
    "class NonLinear(nn.Module):\n",
    "    def __init__(self, inp, output_size, hidden=None):\n",
    "        super(NonLinear, self).__init__()\n",
    "\n",
    "        if hidden is None: hidden = inp\n",
    "        self.layer1 = Linear(inp, hidden, init=\"relu\")\n",
    "        self.layer2 = Linear(hidden, output_size, init=\"final\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "\n",
    "    def zero_init(self):\n",
    "        nn.init.zeros_(self.layer2.weight)\n",
    "        nn.init.zeros_(self.layer2.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cfe155-e04a-49a7-8915-88d4d379d8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SE3InvariantKernel(nn.Module):\n",
    "    def __init__(self, pair_dim, num_pair, num_kernel, std_width=1.0, start=0.0, stop=9.0):\n",
    "        super(SE3InvariantKernel, self).__init__()\n",
    "        \n",
    "        self.num_kernel = num_kernel\n",
    "        self.gaussian = GaussianKernel(\n",
    "            self.num_kernel,\n",
    "            num_pair,\n",
    "            std_width=std_width,\n",
    "            start=start,\n",
    "            stop=stop,\n",
    "        )\n",
    "        self.out_proj = NonLinear(self.num_kernel, pair_dim)\n",
    "\n",
    "    def forward(self, dist, node_type_edge):\n",
    "        edge_feature = self.gaussian(\n",
    "            dist,\n",
    "            node_type_edge.long(),\n",
    "        )\n",
    "        edge_feature = self.out_proj(edge_feature)\n",
    "\n",
    "        return edge_feature\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
